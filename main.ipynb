{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercice 2: _Extraction de DonnÃ©es en Ligne_\n",
    "**Objectif:** Ecrivons les scripts d'extraction de donnÃ©es depuis diffÃ©rentes plateformes."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:20:12.127540Z",
     "start_time": "2025-03-27T18:20:11.548741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from urllib.request import urlopen\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from googlesearch import search\n",
    "import praw\n",
    "import wikipediaapi\n",
    "\n",
    "config = dotenv_values(\".env\")"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. **Amazon:** Web scraping avec BeautifulSoup: Produits, prix, avis",
   "id": "4ed4429937c283cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:26:37.810908Z",
     "start_time": "2025-03-27T17:26:33.800007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"https://www.amazon.com/dp/B0CP22DQQS?th=1\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ],
   "id": "555d354fdf7d4fcd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:26:38.753281Z",
     "start_time": "2025-03-27T17:26:38.697199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "title = soup.find(id=\"productTitle\").get_text(strip=True)\n",
    "price = soup.find('span', {'class': 'a-price'}).get_text(strip=True).split('$')[1]\n",
    "rating = soup.find(class_=\"a-icon-alt\").get_text()\n",
    "print(f\"Produit: {title}\\nPrix: {price}$\\nAvis: {rating}\")"
   ],
   "id": "9e03dc510d3c025f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produit: Marsail Ergonomic Office Chair: Office Computer Desk Chair with High Back Mesh and Adjustable Lumbar Support Rolling Work Swivel Task Chairs with Wheel 3D Armrests and Headrest\n",
      "Prix: 105.58$\n",
      "Avis: 4.4 out of 5 stars\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 2. **Twitter:** Utilisation de l'API Twitter v2: Tweets, likes, retweets",
   "id": "8f7afba164c3e3ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:26:41.458449Z",
     "start_time": "2025-03-27T17:26:39.793794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_url=\"https://api.twitter.com/2\"\n",
    "\n",
    "X_BEARER_TOKEN = config.get(\"X_BEARER_TOKEN\")\n",
    "\n",
    "headers = {\n",
    "  'Authorization': f\"Bearer {X_BEARER_TOKEN}\"\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'tweet.fields': 'created_at,public_metrics',\n",
    "}\n",
    "\n",
    "if not X_BEARER_TOKEN:\n",
    "   raise Exception(\"Bearer token non trouvÃ©. Merci de fournir un valeur Ã  la variable d'environnement X_BEARER_TOKEN.\")\n",
    "\n",
    "def get_tweets(username: str):\n",
    "    response = get(f\"{x_url}/tweets/search/recent?query=from:{username}\", headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Erreur {response.status_code}: {response.text}\")\n",
    "    data = response.json()\n",
    "    return data.get(\"data\")\n",
    "\n",
    "df = pd.DataFrame(get_tweets(\"DylanCalluy\"))\n",
    "print(df.head())"
   ],
   "id": "d267db7405888f3d",
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Erreur 429: {\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m     data \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 23\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(get_tweets(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDylanCalluy\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(df\u001B[38;5;241m.\u001B[39mhead())\n",
      "Cell \u001B[0;32mIn[4], line 19\u001B[0m, in \u001B[0;36mget_tweets\u001B[0;34m(username)\u001B[0m\n\u001B[1;32m     17\u001B[0m response \u001B[38;5;241m=\u001B[39m get(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/tweets/search/recent?query=from:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00musername\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, headers\u001B[38;5;241m=\u001B[39mheaders, params\u001B[38;5;241m=\u001B[39mparams)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[0;32m---> 19\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mErreur \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     20\u001B[0m data \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mException\u001B[0m: Erreur 429: {\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 3. **Instagram:** API Instagram Graph: Captions, likes, images",
   "id": "38d8638768c7808e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:26:41.485354914Z",
     "start_time": "2025-03-27T15:37:52.724131Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "209dd8387678c28f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. **YouTube:** API YouTube Data: Titres, vues, commentaires",
   "id": "b3e337564432dc6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:36:08.768887Z",
     "start_time": "2025-03-27T17:36:02.957982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "YOUTUBE_API_KEY = config.get(\"YOUTUBE_API_KEY\")\n",
    "if not YOUTUBE_API_KEY:\n",
    "   raise Exception(\"API Key non trouvÃ©. Merci de fournir un valeur Ã  la variable d'environnement YOUTUBE_API_KEY.\")\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
    "\n",
    "def get_videos(channel_id: str):\n",
    "    request = youtube.search().list(part=\"snippet\", channelId=channel_id, maxResults=10)\n",
    "    response = request.execute()\n",
    "    videos = []\n",
    "    for item in response['items']:\n",
    "        if item['id']['kind'] == 'youtube#video':\n",
    "            video_id = item['id']['videoId']\n",
    "            video_details = youtube.videos().list(part=\"snippet,statistics\", id=video_id).execute()\n",
    "            video_info = video_details['items'][0]\n",
    "            comments_request = youtube.commentThreads().list(part=\"snippet\", videoId=video_id, maxResults=10)\n",
    "            comments_response = comments_request.execute()\n",
    "            comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay'] for comment in comments_response['items']]\n",
    "            video_data = {\n",
    "                'title': video_info['snippet']['title'],\n",
    "                'views': video_info['statistics']['viewCount'],\n",
    "                'comments': comments\n",
    "            }\n",
    "            videos.append(video_data)\n",
    "    return videos\n",
    "\n",
    "videos = get_videos(\"UCWedHS9qKebauVIK2J7383g\")\n",
    "df = pd.DataFrame(videos)\n",
    "print(df.head())"
   ],
   "id": "255eaae2102bcc23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title    views  \\\n",
      "0  VidÃ©o complÃ¨te sur la chaÃ®ne ! Backstage de no...    13627   \n",
      "1  VidÃ©o complÃ¨te sur la chaÃ®ne ! Q&A exclusive :...  1629471   \n",
      "2            Le rÃ´le abject dâ€™un flic sur le Darkweb   825384   \n",
      "3  La trouvaille scandaleuse d'un hacker sur un d...   383540   \n",
      "4  La trouvaille scandaleuse d'un physicien sur u...   607457   \n",
      "\n",
      "                                            comments  \n",
      "0  [Le futur hugo dÃ©crypte ? ðŸ˜‰ðŸ˜‰, il a un appareil...  \n",
      "1  [FormÃ© Ã  lâ€™esprit critique mais bien encadrÃ© p...  \n",
      "2  [Christophe coulons, Il a dÃ©crochÃ© le job car ...  \n",
      "3  [Depuis quand tu prends autant de drogue dâ€™un ...  \n",
      "4  [Super vidÃ©o comme d&#39;habitude mais je me p...  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 5. **Google Search:** Scraper avec googlesearch (RÃ©sultats de recherche)",
   "id": "3c53c595a8e02841"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:01:20.319736Z",
     "start_time": "2025-03-27T18:01:19.313596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def google_search(query, num_results=10):\n",
    "    search_results = []\n",
    "    for result in search(query, num_results=num_results):\n",
    "        search_results.append(result)\n",
    "    return search_results\n",
    "\n",
    "query = 'Data Science Course site:*.edu filetype:pdf intext:\"Book\"'\n",
    "results = google_search(query)\n",
    "df = pd.DataFrame(results)\n",
    "print(df.head())"
   ],
   "id": "71c4bd5e6c92407d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0            https://www.cs.cornell.edu/jeh/book.pdf\n",
      "1  https://www.webpages.uidaho.edu/~stevel/517/Th...\n",
      "2  https://cims.nyu.edu/~cfgranda/pages/stuff/pro...\n",
      "3  https://digital.library.ncat.edu/cgi/viewconte...\n",
      "4  https://conocer.cide.edu/default.aspx/libweb/4...\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6. **Reddit:** API Reddit (PRAW). (Posts, votes, commentaires)",
   "id": "124cfe4cab235f52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:01:45.279104Z",
     "start_time": "2025-03-27T18:01:24.419958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=config.get(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=config.get(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent=config.get(\"REDDIT_USER_AGENT\")\n",
    ")\n",
    "\n",
    "def get_reddit_posts(subreddit_name, limit=10):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    for post in subreddit.hot(limit=limit):\n",
    "        post_data = {\n",
    "            'title': post.title,\n",
    "            'score': post.score,\n",
    "            'num_comments': post.num_comments,\n",
    "            'comments': [comment.body for comment in post.comments[:10]]\n",
    "        }\n",
    "        posts.append(post_data)\n",
    "    return posts\n",
    "\n",
    "subreddit_name = \"learnpython\"\n",
    "posts = get_reddit_posts(subreddit_name)\n",
    "df = pd.DataFrame(posts)\n",
    "print(df.head())"
   ],
   "id": "8d65b5f88332f07d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          title  score  num_comments  \\\n",
      "0           Ask Anything Monday - Weekly Thread     10            28   \n",
      "1           How do you actually learn by doing?    101            49   \n",
      "2         Build a to-do-list program (beginner)      7             2   \n",
      "3  Why are some types made immutable in Python?      2             0   \n",
      "4  Whatâ€™s the best application to learn python?      5            17   \n",
      "\n",
      "                                            comments  \n",
      "0  [Have you ever developed a \"great new idea\" in...  \n",
      "1  [As someone who has used Python for more than ...  \n",
      "2  [Next improvement would be to save & load list...  \n",
      "3                                                 []  \n",
      "4  [I liked the 100 days of code course on udemy,...  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7. **Wikipedia:** API WikipÃ©dia (Contenu d'articles).",
   "id": "c4cd10cbd0a5a3c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:23:13.096962Z",
     "start_time": "2025-03-27T18:23:10.313783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_wikipedia_content(page_name):\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(user_agent='data_science_exo')\n",
    "    page = wiki_wiki.page(page_name)\n",
    "    if page.exists():\n",
    "        return {\n",
    "            'title': page.title,\n",
    "            'summary': page.summary,\n",
    "            'full_content': page.text\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "page_name = \"Dunningâ€“Kruger_effect\"\n",
    "content = get_wikipedia_content(page_name)\n",
    "if content:\n",
    "    df = pd.DataFrame([content])\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(f\"Page '{page_name}' does not exist.\")"
   ],
   "id": "d38d1f41e03f9bce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   title                                            summary  \\\n",
      "0  Dunningâ€“Kruger effect  The Dunningâ€“Kruger effect is a cognitive bias ...   \n",
      "\n",
      "                                        full_content  \n",
      "0  The Dunningâ€“Kruger effect is a cognitive bias ...  \n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
